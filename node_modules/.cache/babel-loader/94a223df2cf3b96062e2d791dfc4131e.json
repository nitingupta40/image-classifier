{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { convertToTensor } from '../tensor_util_env';\nimport { maximum, mul } from './binary_ops';\nimport { getReductionAxes } from './broadcast_util';\nimport { where } from './logical_ops';\nimport { op } from './operation';\nimport { SELU_SCALE, SELU_SCALEALPHA } from './selu_util';\nimport { scalar, zerosLike } from './tensor_ops';\n/**\n * Computes rectified linear element-wise: `max(x, 0)`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.relu().print();  // or tf.relu(x)\n * ```\n * @param x The input tensor. If the dtype is `bool`, the output dtype will be\n *     `int32'.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\nfunction relu_(x) {\n  const $x = convertToTensor(x, 'x', 'relu');\n\n  if ($x.dtype === 'bool') {\n    return $x.toInt();\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved; // tslint:disable-next-line: no-unnecessary-type-assertion\n\n    return {\n      x: () => mul(dy, $x.step().toFloat())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.relu($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Relu');\n}\n/**\n * Computes rectified linear 6 element-wise: `min(max(x, 0), 6)`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 8]);\n *\n * x.relu6().print();  // or tf.relu6(x)\n * ```\n * @param x The input tensor. If the dtype is `bool`, the output dtype will be\n *     `int32'.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction relu6_(x) {\n  const $x = convertToTensor(x, 'x', 'relu6');\n\n  if ($x.dtype === 'bool') {\n    return $x.toInt();\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    const mask = $x.lessEqual(6).mul($x.step()); // tslint:disable-next-line: no-unnecessary-type-assertion\n\n    return {\n      x: () => mul(dy, mask.toFloat())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.relu6($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Relu6');\n}\n/**\n * Computes exponential linear element-wise: `x > 0 ? e ^ x - 1 : 0`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 1, -3, 2]);\n *\n * x.elu().print();  // or tf.elu(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction elu_(x) {\n  const $x = convertToTensor(x, 'x', 'elu');\n\n  const grad = (dy, saved) => {\n    const [y] = saved;\n    return {\n      $x: () => ENGINE.runKernelFunc(backend => backend.eluDer(dy, y), {\n        dy,\n        y\n      })\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.elu($x);\n    save([y]);\n    return y;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes scaled exponential linear element-wise.\n *\n * `x < 0 ? scale * alpha * (exp(x) - 1) : x`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.selu().print();  // or tf.selu(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction selu_(x) {\n  const $x = convertToTensor(x, 'x', 'selu');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => {\n        const mask = $x.greater(scalar(0));\n        const scaleAlpha = scalar(SELU_SCALEALPHA);\n        const scale = scalar(SELU_SCALE);\n        const greaterThanZeroDer = dy.mul(scale);\n        const lessEqualZeroDer = dy.mul(scaleAlpha).mul($x.toFloat().exp());\n        return where(mask, greaterThanZeroDer, lessEqualZeroDer);\n      }\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.selu($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes leaky rectified linear element-wise.\n *\n * See\n * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n * ```\n * @param x The input tensor.\n * @param alpha The scaling factor for negative values, defaults to 0.2.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction leakyRelu_(x, alpha = 0.2) {\n  const $x = convertToTensor(x, 'x', 'leakyRelu');\n  return maximum(scalar(alpha).mul($x), $x);\n}\n/**\n * Computes leaky rectified linear element-wise with parametric alphas.\n *\n * `x < 0 ? alpha * x : f(x) = x`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n * const alpha = tf.scalar(0.1);\n *\n * x.prelu(alpha).print();  // or tf.prelu(x, alpha)\n * ```\n * @param x The input tensor.\n * @param alpha Scaling factor for negative values.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction prelu_(x, alpha) {\n  const $x = convertToTensor(x, 'x', 'prelu');\n  const $alpha = convertToTensor(alpha, 'alpha', 'prelu');\n\n  const grad = (dy, saved) => {\n    const [$x, $alpha] = saved;\n    const mask = $x.greater(0);\n    return {\n      x: () => where(mask, dy, dy.mul($alpha)),\n      alpha: () => {\n        let res = where(mask, zerosLike(dy), dy.mul($x));\n        const reduceAxes = getReductionAxes($alpha.shape, dy.shape);\n\n        if (reduceAxes.length > 0) {\n          res = res.sum(reduceAxes);\n        }\n\n        return res.reshape($alpha.shape);\n      }\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.prelu($x, $alpha);\n    save([$x, $alpha]);\n    return res;\n  }, {\n    x: $x,\n    alpha: $alpha\n  }, grad, 'Prelu');\n}\n\nexport const elu = op({\n  elu_\n});\nexport const leakyRelu = op({\n  leakyRelu_\n});\nexport const prelu = op({\n  prelu_\n});\nexport const relu = op({\n  relu_\n});\nexport const relu6 = op({\n  relu6_\n});\nexport const selu = op({\n  selu_\n});","map":{"version":3,"sources":["../../src/ops/relu_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AAEA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,SAAQ,OAAR,EAAiB,GAAjB,QAA2B,cAA3B;AACA,SAAQ,gBAAR,QAA+B,kBAA/B;AACA,SAAQ,KAAR,QAAoB,eAApB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,UAAR,EAAoB,eAApB,QAA0C,aAA1C;AACA,SAAQ,MAAR,EAAgB,SAAhB,QAAgC,cAAhC;AAEA;;;;;;;;;;;;AAWA;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,WAAO,EAAE,CAAC,KAAH,EAAP;AACD;;AACD,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb,CADsC,CAEtC;;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,GAAG,CAAC,EAAD,EAAK,EAAE,CAAC,IAAH,GAAU,OAAV,EAAL;AAAb,KAAP;AACD,GAJD;;AAKA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,MAJX,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,WAAO,EAAE,CAAC,KAAH,EAAP;AACD;;AACD,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,UAAM,IAAI,GAAG,EAAE,CAAC,SAAH,CAAa,CAAb,EAAgB,GAAhB,CAAoB,EAAE,CAAC,IAAH,EAApB,CAAb,CAFsC,CAGtC;;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,GAAG,CAAC,EAAD,EAAK,IAAI,CAAC,OAAL,EAAL;AAAb,KAAP;AACD,GALD;;AAMA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,OAJX,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,CAAD,IAAM,KAAZ;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MACA,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,MAAR,CAAe,EAAf,EAAmB,CAAnB,CAAhC,EAAuD;AAAC,QAAA,EAAD;AAAK,QAAA;AAAL,OAAvD;AAFC,KAAP;AAID,GAND;;AAOA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,CAAC,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;;AAYA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MAAK;AACP,cAAM,IAAI,GAAG,EAAE,CAAC,OAAH,CAAW,MAAM,CAAC,CAAD,CAAjB,CAAb;AAEA,cAAM,UAAU,GAAG,MAAM,CAAC,eAAD,CAAzB;AACA,cAAM,KAAK,GAAG,MAAM,CAAC,UAAD,CAApB;AAEA,cAAM,kBAAkB,GAAG,EAAE,CAAC,GAAH,CAAO,KAAP,CAA3B;AACA,cAAM,gBAAgB,GAAG,EAAE,CAAC,GAAH,CAAO,UAAP,EAAmB,GAAnB,CAAuB,EAAE,CAAC,OAAH,GAAa,GAAb,EAAvB,CAAzB;AAEA,eAAO,KAAK,CAAC,IAAD,EAAO,kBAAP,EAA2B,gBAA3B,CAAZ;AACD;AAXI,KAAP;AAaD,GAfD;;AAgBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;;;;;AAeA;;;AACA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,KAAK,GAAG,GAA/D,EAAkE;AAChE,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,SAAO,OAAO,CAAC,MAAM,CAAC,KAAD,CAAN,CAAc,GAAd,CAAkB,EAAlB,CAAD,EAAwB,EAAxB,CAAd;AACD;AAED;;;;;;;;;;;;;;;AAcA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAmD,KAAnD,EAAsE;AACpE,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;AACA,QAAM,MAAM,GAAG,eAAe,CAAC,KAAD,EAAQ,OAAR,EAAiB,OAAjB,CAA9B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC3C,UAAM,CAAC,EAAD,EAAK,MAAL,IAAe,KAArB;AACA,UAAM,IAAI,GAAG,EAAE,CAAC,OAAH,CAAW,CAAX,CAAb;AAEA,WAAO;AACL,MAAA,CAAC,EAAE,MAAM,KAAK,CAAC,IAAD,EAAO,EAAP,EAAW,EAAE,CAAC,GAAH,CAAO,MAAP,CAAX,CADT;AAEL,MAAA,KAAK,EAAE,MAAK;AACV,YAAI,GAAG,GAAG,KAAK,CAAC,IAAD,EAAO,SAAS,CAAC,EAAD,CAAhB,EAAsB,EAAE,CAAC,GAAH,CAAO,EAAP,CAAtB,CAAf;AACA,cAAM,UAAU,GAAG,gBAAgB,CAAC,MAAM,CAAC,KAAR,EAAe,EAAE,CAAC,KAAlB,CAAnC;;AACA,YAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,UAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,eAAO,GAAG,CAAC,OAAJ,CAAY,MAAM,CAAC,KAAnB,CAAP;AACD;AATI,KAAP;AAWD,GAfD;;AAiBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,EAAkB,MAAlB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,MAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,EAAJ;AAAQ,IAAA,KAAK,EAAE;AAAf,GAJI,EAIoB,IAJpB,EAI0B,OAJ1B,CAAP;AAKD;;AAED,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { convertToTensor } from '../tensor_util_env';\nimport { maximum, mul } from './binary_ops';\nimport { getReductionAxes } from './broadcast_util';\nimport { where } from './logical_ops';\nimport { op } from './operation';\nimport { SELU_SCALE, SELU_SCALEALPHA } from './selu_util';\nimport { scalar, zerosLike } from './tensor_ops';\n/**\n * Computes rectified linear element-wise: `max(x, 0)`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.relu().print();  // or tf.relu(x)\n * ```\n * @param x The input tensor. If the dtype is `bool`, the output dtype will be\n *     `int32'.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction relu_(x) {\n    const $x = convertToTensor(x, 'x', 'relu');\n    if ($x.dtype === 'bool') {\n        return $x.toInt();\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { x: () => mul(dy, $x.step().toFloat()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.relu($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Relu');\n}\n/**\n * Computes rectified linear 6 element-wise: `min(max(x, 0), 6)`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 8]);\n *\n * x.relu6().print();  // or tf.relu6(x)\n * ```\n * @param x The input tensor. If the dtype is `bool`, the output dtype will be\n *     `int32'.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction relu6_(x) {\n    const $x = convertToTensor(x, 'x', 'relu6');\n    if ($x.dtype === 'bool') {\n        return $x.toInt();\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        const mask = $x.lessEqual(6).mul($x.step());\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { x: () => mul(dy, mask.toFloat()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.relu6($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Relu6');\n}\n/**\n * Computes exponential linear element-wise: `x > 0 ? e ^ x - 1 : 0`.\n *\n * ```js\n * const x = tf.tensor1d([-1, 1, -3, 2]);\n *\n * x.elu().print();  // or tf.elu(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction elu_(x) {\n    const $x = convertToTensor(x, 'x', 'elu');\n    const grad = (dy, saved) => {\n        const [y] = saved;\n        return {\n            $x: () => ENGINE.runKernelFunc(backend => backend.eluDer(dy, y), { dy, y })\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.elu($x);\n        save([y]);\n        return y;\n    }, { $x }, grad);\n}\n/**\n * Computes scaled exponential linear element-wise.\n *\n * `x < 0 ? scale * alpha * (exp(x) - 1) : x`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.selu().print();  // or tf.selu(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction selu_(x) {\n    const $x = convertToTensor(x, 'x', 'selu');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            $x: () => {\n                const mask = $x.greater(scalar(0));\n                const scaleAlpha = scalar(SELU_SCALEALPHA);\n                const scale = scalar(SELU_SCALE);\n                const greaterThanZeroDer = dy.mul(scale);\n                const lessEqualZeroDer = dy.mul(scaleAlpha).mul($x.toFloat().exp());\n                return where(mask, greaterThanZeroDer, lessEqualZeroDer);\n            }\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.selu($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes leaky rectified linear element-wise.\n *\n * See\n * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n * ```\n * @param x The input tensor.\n * @param alpha The scaling factor for negative values, defaults to 0.2.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction leakyRelu_(x, alpha = 0.2) {\n    const $x = convertToTensor(x, 'x', 'leakyRelu');\n    return maximum(scalar(alpha).mul($x), $x);\n}\n/**\n * Computes leaky rectified linear element-wise with parametric alphas.\n *\n * `x < 0 ? alpha * x : f(x) = x`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n * const alpha = tf.scalar(0.1);\n *\n * x.prelu(alpha).print();  // or tf.prelu(x, alpha)\n * ```\n * @param x The input tensor.\n * @param alpha Scaling factor for negative values.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction prelu_(x, alpha) {\n    const $x = convertToTensor(x, 'x', 'prelu');\n    const $alpha = convertToTensor(alpha, 'alpha', 'prelu');\n    const grad = (dy, saved) => {\n        const [$x, $alpha] = saved;\n        const mask = $x.greater(0);\n        return {\n            x: () => where(mask, dy, dy.mul($alpha)),\n            alpha: () => {\n                let res = where(mask, zerosLike(dy), dy.mul($x));\n                const reduceAxes = getReductionAxes($alpha.shape, dy.shape);\n                if (reduceAxes.length > 0) {\n                    res = res.sum(reduceAxes);\n                }\n                return res.reshape($alpha.shape);\n            }\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.prelu($x, $alpha);\n        save([$x, $alpha]);\n        return res;\n    }, { x: $x, alpha: $alpha }, grad, 'Prelu');\n}\nexport const elu = op({ elu_ });\nexport const leakyRelu = op({ leakyRelu_ });\nexport const prelu = op({ prelu_ });\nexport const relu = op({ relu_ });\nexport const relu6 = op({ relu6_ });\nexport const selu = op({ selu_ });\n//# sourceMappingURL=relu_ops.js.map"]},"metadata":{},"sourceType":"module"}