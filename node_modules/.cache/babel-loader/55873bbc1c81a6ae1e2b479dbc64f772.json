{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { deprecationWarn } from '../globals';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { add } from './add';\nimport * as broadcast_util from './broadcast_util';\nimport { op } from './operation';\nimport { scalar, zerosLike } from './tensor_ops';\nimport { neg } from './unary_ops';\n/**\n * @deprecated\n * Adds two `tf.Tensor`s element-wise, A + B.\n *\n * Inputs must be the same shape. For broadcasting support, use add() instead.\n *\n * @param a The first Tensor to add element-wise.\n * @param b The second Tensor to add element-wise.\n */\n\nfunction addStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'addStrict');\n  const $b = convertToTensor(b, 'b', 'addStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in addStrict: ');\n  return $a.add($b);\n}\n/**\n * @deprecated\n * Subtracts two `tf.Tensor`s element-wise, A - B. Inputs must\n * be the same shape.\n *\n * For broadcasting support, use `tf.sub` instead.\n *\n * @param a The first Tensor to subtract element-wise.\n * @param b The second Tensor to subtract element-wise.\n */\n\n\nfunction subStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'subStrict');\n  const $b = convertToTensor(b, 'b', 'subStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in subStrict: ');\n  return $a.sub($b);\n}\n/**\n * Computes the power of one `tf.Tensor` to another. Supports broadcasting.\n *\n * Given a `tf.Tensor` x and a `tf.Tensor` y, this operation computes x^y for\n * corresponding elements in x and y. The result's dtype will be the upcasted\n * type of the `base` and `exp` dtypes.\n *\n * ```js\n * const a = tf.tensor([[2, 3], [4, 5]])\n * const b = tf.tensor([[1, 2], [3, 0]]).toInt();\n *\n * a.pow(b).print();  // or tf.pow(a, b)\n * ```\n *\n * ```js\n * const a = tf.tensor([[1, 2], [3, 4]])\n * const b = tf.tensor(2).toInt();\n *\n * a.pow(b).print();  // or tf.pow(a, b)\n * ```\n * We also expose `powStrict` which has the same signature as this op and\n * asserts that `base` and `exp` are the same shape (does not broadcast).\n *\n * @param base The base `tf.Tensor` to pow element-wise.\n * @param exp The exponent `tf.Tensor` to pow element-wise.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction pow_(base, exp) {\n  let $base = convertToTensor(base, 'base', 'pow');\n  let $exp = convertToTensor(exp, 'exp', 'pow');\n  [$base, $exp] = makeTypesMatch($base, $exp);\n  const outShape = broadcast_util.assertAndGetBroadcastShape($base.shape, $exp.shape);\n\n  const grad = (dy, saved) => {\n    const [$base, $exp, y] = saved;\n\n    const derBase = () => {\n      const expFloat = $exp.toFloat();\n      let res = dy.mul(expFloat.mul($base.pow(expFloat.sub(scalar(1)))));\n      const reduceAxes = broadcast_util.getReductionAxes($base.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes);\n      }\n\n      return res.reshape($base.shape);\n    };\n\n    const derExp = () => {\n      const condition = $base.greater(0);\n      const logBase = $base.log().where(condition, zerosLike($base));\n      let res = dy.mul(y.mul(logBase));\n      const reduceAxes = broadcast_util.getReductionAxes($exp.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes);\n      }\n\n      return res.reshape($exp.shape);\n    };\n\n    return {\n      a: derBase,\n      b: derExp\n    };\n  };\n\n  const attrs = {};\n  const inputsToSave = [$base, $exp];\n  const outputsToSave = [true];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.pow($base, $exp);\n    save([$base, $exp, y]);\n    return y;\n  }, {\n    a: $base,\n    b: $exp\n  }, grad, 'Pow', attrs, inputsToSave, outputsToSave);\n}\n/**\n * @deprecated\n * Computes the power of one `tf.Tensor` to another. Inputs must\n * be the same shape.\n *\n * For broadcasting support, use `tf.pow` instead.\n *\n * @param base The base tensor to pow element-wise.\n * @param exp The exponent tensor to pow element-wise.\n */\n\n\nfunction powStrict_(base, exp) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  util.assertShapesMatch(base.shape, exp.shape, 'Error in powStrict: ');\n  return base.pow(exp);\n}\n/**\n * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.\n *\n * We also expose `tf.mulStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.tensor1d([2, 3, 4, 5]);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n *\n * ```js\n * // Broadcast mul a with b.\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.scalar(5);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n * @param a The first tensor to multiply.\n * @param b The second tensor to multiply. Must have the same dtype as `a`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction mul_(a, b) {\n  let $a = convertToTensor(a, 'a', 'mul');\n  let $b = convertToTensor(b, 'b', 'mul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => {\n      const res = dy.mul($b.toFloat());\n      const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        return res.sum(reduceAxes).reshape($a.shape);\n      }\n\n      return res;\n    };\n\n    const derB = () => {\n      const res = dy.mul($a.toFloat());\n      const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        return res.sum(reduceAxes).reshape($b.shape);\n      }\n\n      return res;\n    };\n\n    return {\n      a: derA,\n      b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.multiply($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    a: $a,\n    b: $b\n  }, der, 'Mul');\n}\n/**\n * @deprecated\n * Multiplies two `tf.Tensor`s element-wise, A * B.\n *\n * Inputs must be the same shape. For broadcasting support, use `tf.mul`.\n *\n * @param a The first tensor to multiply.\n * @param b The first tensor to multiply. Must have the same\n *    dtype as `a`.\n */\n\n\nfunction mulStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'mul');\n  const $b = convertToTensor(b, 'b', 'mul');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in multiplyStrict: ');\n  return $a.mul($b);\n}\n/**\n * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.\n * The result is rounded with floor function.\n *\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 9, 16]);\n * const b = tf.tensor1d([1, 2, 3, 4]);\n *\n * a.floorDiv(b).print();  // or tf.div(a, b)\n * ```\n *\n * ```js\n * // Broadcast div a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(2);\n *\n * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n * ```\n *\n * @param a The first tensor as the numerator.\n * @param b The second tensor as the denominator. Must have the same dtype as\n * `a`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction floorDiv_(a, b) {\n  let $a = convertToTensor(a, 'a', 'floorDiv');\n  let $b = convertToTensor(b, 'b', 'floorDiv');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => {\n      const res = dy.div($b.toFloat());\n      const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        return res.sum(reduceAxes).reshape($a.shape);\n      }\n\n      return res;\n    };\n\n    const derB = () => {\n      let res = dy.mul($a.toFloat());\n      const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes).reshape($b.shape);\n      }\n\n      const tmp = $b.square();\n      return res.div(tmp.toFloat()).neg();\n    };\n\n    return {\n      a: derA,\n      b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.floorDiv($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    a: $a,\n    b: $b\n  }, der, 'FloorDiv');\n}\n/**\n * @deprecated\n * Divides two `tf.Tensor`s element-wise, A / B. Inputs must\n * be the same shape.\n *\n * @param a The first tensor as the numerator for element-wise division.\n * @param b The second tensor as the denominator for element-wise division.\n */\n\n\nfunction divStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'div');\n  const $b = convertToTensor(b, 'b', 'div');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in divideStrict: ');\n  return $a.div($b);\n}\n/**\n * Returns the mod of a and b element-wise.\n * `floor(x / y) * y + mod(x, y) = x`\n * Supports broadcasting.\n *\n * We also expose `tf.modStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * ```js\n * // Broadcast a mod b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction mod_(a, b) {\n  let $a = convertToTensor(a, 'a', 'mod');\n  let $b = convertToTensor(b, 'b', 'mod');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => {\n      const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        return dy.sum(reduceAxes).reshape($a.shape);\n      }\n\n      return dy;\n    };\n\n    const derB = () => {\n      const res = dy.mul($a.div($b).floor().neg());\n      const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        return res.sum(reduceAxes).reshape($b.shape);\n      }\n\n      return res;\n    };\n\n    return {\n      $a: derA,\n      $b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.mod($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    $a,\n    $b\n  }, der);\n}\n/**\n * @deprecated\n * Returns the mod of a and b (`a < b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use mod().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\n\n\nfunction modStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'modStrict');\n  const $b = convertToTensor(b, 'b', 'modStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in modStrict: ');\n  return $a.mod($b);\n}\n/**\n * Returns the min of a and b (`a < b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `minimumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * ```js\n * // Broadcast minimum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction minimum_(a, b) {\n  let $a = convertToTensor(a, 'a', 'minimum');\n  let $b = convertToTensor(b, 'b', 'minimum');\n  [$a, $b] = makeTypesMatch($a, $b);\n\n  if ($a.dtype === 'bool') {\n    $a = $a.toInt();\n    $b = $b.toInt();\n  }\n\n  broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => dy.mul($a.lessEqual($b).toFloat());\n\n    const derB = () => dy.mul($a.greater($b).toFloat());\n\n    return {\n      a: derA,\n      b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.minimum($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    a: $a,\n    b: $b\n  }, der, 'Minimum');\n}\n/**\n * @deprecated\n * Returns the min of a and b (`a < b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use minimum().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\n\n\nfunction minimumStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'minimumStrict');\n  const $b = convertToTensor(b, 'b', 'minimumStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in minimumStrict: ');\n  return $a.minimum($b);\n}\n/**\n * Returns the max of a and b (`a > b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `tf.maximumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * ```js\n * // Broadcast maximum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\n\n\nfunction maximum_(a, b) {\n  let $a = convertToTensor(a, 'a', 'maximum');\n  let $b = convertToTensor(b, 'b', 'maximum');\n  [$a, $b] = makeTypesMatch($a, $b);\n\n  if ($a.dtype === 'bool') {\n    $a = $a.toInt();\n    $b = $b.toInt();\n  }\n\n  broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => dy.mul($a.greaterEqual($b).toFloat());\n\n    const derB = () => dy.mul($a.less($b).toFloat());\n\n    return {\n      a: derA,\n      b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.maximum($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    a: $a,\n    b: $b\n  }, der, 'Maximum');\n}\n/**\n * @deprecated\n * Returns the max of a and b (`a > b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use maximum().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\n\n\nfunction maximumStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'maximumStrict');\n  const $b = convertToTensor(b, 'b', 'maximumStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in maximumStrict: ');\n  return $a.maximum($b);\n}\n/**\n * @deprecated\n * Returns (a - b) * (a - b) element-wise.\n *\n * Inputs must be the same shape. For broadcasting support, use\n * `tf.squaredDifference` instead.\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n\n\nfunction squaredDifferenceStrict_(a, b) {\n  deprecationWarn('strict variants of ops have been deprecated ' + 'and will be removed in future');\n  const $a = convertToTensor(a, 'a', 'squaredDifferenceStrict');\n  const $b = convertToTensor(b, 'b', 'squaredDifferenceStrict');\n  util.assertShapesMatch($a.shape, $b.shape, 'Error in squaredDifferenceStrict: ');\n  return $a.squaredDifference($b);\n}\n/**\n * Computes arctangent of `tf.Tensor`s a / b element-wise: `atan2(a, b)`.\n * Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1.0, 1.0, -1.0, .7]);\n * const b = tf.tensor1d([2.0, 13.0, 3.5, .21]);\n *\n * tf.atan2(a, b).print()\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n *\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction atan2_(a, b) {\n  let $a = convertToTensor(a, 'a', 'atan2');\n  let $b = convertToTensor(b, 'b', 'atan2');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n\n  const der = (dy, saved) => {\n    const [$a, $b] = saved;\n\n    const derA = () => {\n      const d = add($a.square(), $b.square());\n      let res = dy.mul($b.div(d));\n      const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes);\n      }\n\n      return res.reshape($a.shape);\n    };\n\n    const derB = () => {\n      const d = add($a.square(), $b.square());\n      let res = neg(dy.mul($a.div(d)));\n      const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n\n      if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes);\n      }\n\n      return res.reshape($b.shape);\n    };\n\n    return {\n      $a: derA,\n      $b: derB\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.atan2($a, $b);\n    save([$a, $b]);\n    return res;\n  }, {\n    $a,\n    $b\n  }, der);\n}\n\nexport const addStrict = op({\n  addStrict_\n});\nexport const atan2 = op({\n  atan2_\n});\nexport const divStrict = op({\n  divStrict_\n});\nexport const floorDiv = op({\n  floorDiv_\n});\nexport const maximum = op({\n  maximum_\n});\nexport const maximumStrict = op({\n  maximumStrict_\n});\nexport const minimum = op({\n  minimum_\n});\nexport const minimumStrict = op({\n  minimumStrict_\n});\nexport const mod = op({\n  mod_\n});\nexport const modStrict = op({\n  modStrict_\n});\nexport const mul = op({\n  mul_\n});\nexport const mulStrict = op({\n  mulStrict_\n});\nexport const pow = op({\n  pow_\n});\nexport const powStrict = op({\n  powStrict_\n});\nexport const squaredDifferenceStrict = op({\n  squaredDifferenceStrict_\n});\nexport const subStrict = op({\n  subStrict_\n});","map":{"version":3,"sources":["../../src/ops/binary_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,eAAR,QAA8B,YAA9B;AAEA,SAAQ,cAAR,QAA6B,gBAA7B;AACA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,SAAtB;AAEA,SAAQ,GAAR,QAAkB,OAAlB;AACA,OAAO,KAAK,cAAZ,MAAgC,kBAAhC;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,MAAR,EAAgB,SAAhB,QAAgC,cAAhC;AACA,SAAQ,GAAR,QAAkB,aAAlB;AAEA;;;;;;;;;;AASA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,CAAvD,EAAsE;AACpE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAGA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,sBAA3C;AACA,SAAO,EAAE,CAAC,GAAH,CAAO,EAAP,CAAP;AACD;AAED;;;;;;;;;;;;AAUA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,CAAvD,EAAsE;AACpE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,sBAA3C;AACA,SAAO,EAAE,CAAC,GAAH,CAAO,EAAP,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA;;;AACA,SAAS,IAAT,CACI,IADJ,EAC6B,GAD7B,EACmD;AACjD,MAAI,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,KAAf,CAA3B;AACA,MAAI,IAAI,GAAG,eAAe,CAAC,GAAD,EAAM,KAAN,EAAa,KAAb,CAA1B;AACA,GAAC,KAAD,EAAQ,IAAR,IAAgB,cAAc,CAAC,KAAD,EAAQ,IAAR,CAA9B;AAEA,QAAM,QAAQ,GACV,cAAc,CAAC,0BAAf,CAA0C,KAAK,CAAC,KAAhD,EAAuD,IAAI,CAAC,KAA5D,CADJ;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC3C,UAAM,CAAC,KAAD,EAAQ,IAAR,EAAc,CAAd,IAAmB,KAAzB;;AACA,UAAM,OAAO,GAAG,MAAK;AACnB,YAAM,QAAQ,GAAG,IAAI,CAAC,OAAL,EAAjB;AACA,UAAI,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,QAAQ,CAAC,GAAT,CAAa,KAAK,CAAC,GAAN,CAAU,QAAQ,CAAC,GAAT,CAAa,MAAM,CAAC,CAAD,CAAnB,CAAV,CAAb,CAAP,CAAV;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,KAAK,CAAC,KAAtC,EAA6C,QAA7C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,aAAO,GAAG,CAAC,OAAJ,CAAY,KAAK,CAAC,KAAlB,CAAP;AACD,KARD;;AASA,UAAM,MAAM,GAAG,MAAK;AAClB,YAAM,SAAS,GAAG,KAAK,CAAC,OAAN,CAAc,CAAd,CAAlB;AACA,YAAM,OAAO,GAAG,KAAK,CAAC,GAAN,GAAY,KAAZ,CAAkB,SAAlB,EAA6B,SAAS,CAAC,KAAD,CAAtC,CAAhB;AACA,UAAI,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,CAAC,CAAC,GAAF,CAAM,OAAN,CAAP,CAAV;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,IAAI,CAAC,KAArC,EAA4C,QAA5C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,aAAO,GAAG,CAAC,OAAJ,CAAY,IAAI,CAAC,KAAjB,CAAP;AACD,KATD;;AAUA,WAAO;AAAC,MAAA,CAAC,EAAE,OAAJ;AAAa,MAAA,CAAC,EAAE;AAAhB,KAAP;AACD,GAtBD;;AAwBA,QAAM,KAAK,GAAG,EAAd;AACA,QAAM,YAAY,GAAG,CAAC,KAAD,EAAQ,IAAR,CAArB;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,CAAC,GAAG,OAAO,CAAC,GAAR,CAAY,KAAZ,EAAmB,IAAnB,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,KAAD,EAAQ,IAAR,EAAc,CAAd,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,KAAJ;AAAW,IAAA,CAAC,EAAE;AAAd,GAJI,EAIiB,IAJjB,EAIuB,KAJvB,EAI8B,KAJ9B,EAIqC,YAJrC,EAImD,aAJnD,CAAP;AAKD;AAED;;;;;;;;;;;;AAUA,SAAS,UAAT,CAAsC,IAAtC,EAA+C,GAA/C,EAA0D;AACxD,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,EAAA,IAAI,CAAC,iBAAL,CAAuB,IAAI,CAAC,KAA5B,EAAmC,GAAG,CAAC,KAAvC,EAA8C,sBAA9C;AACA,SAAO,IAAI,CAAC,GAAL,CAAS,GAAT,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;AAuBA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAAsD,CAAtD,EAA0E;AACxE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,QAAQ,GACV,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD,CADJ;;AAGA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP,CAAZ;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,eAAO,GAAG,CAAC,GAAJ,CAAQ,UAAR,EAAoB,OAApB,CAA4B,EAAE,CAAC,KAA/B,CAAP;AACD;;AACD,aAAO,GAAP;AACD,KAPD;;AAQA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP,CAAZ;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,eAAO,GAAG,CAAC,GAAJ,CAAQ,UAAR,EAAoB,OAApB,CAA4B,EAAE,CAAC,KAA/B,CAAP;AACD;;AACD,aAAO,GAAP;AACD,KAPD;;AAQA,WAAO;AAAC,MAAA,CAAC,EAAE,IAAJ;AAAU,MAAA,CAAC,EAAE;AAAb,KAAP;AACD,GAnBD;;AAoBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,QAAR,CAAiB,EAAjB,EAAqB,EAArB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,EAAJ;AAAQ,IAAA,CAAC,EAAE;AAAX,GAJI,EAIY,GAJZ,EAIiB,KAJjB,CAAP;AAKD;AAED;;;;;;;;;;;;AAUA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,CAAvD,EAAsE;AACpE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,2BAA3C;AACA,SAAO,EAAE,CAAC,GAAH,CAAO,EAAP,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;AAwBA;;;AACA,SAAS,SAAT,CACI,CADJ,EAC0B,CAD1B,EAC8C;AAC5C,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,UAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,UAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,QAAQ,GACV,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD,CADJ;;AAEA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP,CAAZ;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,eAAO,GAAG,CAAC,GAAJ,CAAQ,UAAR,EAAoB,OAApB,CAA4B,EAAE,CAAC,KAA/B,CAAP;AACD;;AACD,aAAO,GAAP;AACD,KAPD;;AAQA,UAAM,IAAI,GAAG,MAAK;AAChB,UAAI,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP,CAAV;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,EAAoB,OAApB,CAA4B,EAAE,CAAC,KAA/B,CAAN;AACD;;AACD,YAAM,GAAG,GAAG,EAAE,CAAC,MAAH,EAAZ;AACA,aAAO,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,OAAJ,EAAR,EAAuB,GAAvB,EAAP;AACD,KARD;;AASA,WAAO;AAAC,MAAA,CAAC,EAAE,IAAJ;AAAU,MAAA,CAAC,EAAE;AAAb,KAAP;AACD,GApBD;;AAqBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,QAAR,CAAiB,EAAjB,EAAqB,EAArB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,EAAJ;AAAQ,IAAA,CAAC,EAAE;AAAX,GAJI,EAIY,GAJZ,EAIiB,UAJjB,CAAP;AAKD;AAED;;;;;;;;;;AAQA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,CAAvD,EAAsE;AACpE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,yBAA3C;AACA,SAAO,EAAE,CAAC,GAAH,CAAO,EAAP,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAAsD,CAAtD,EAA0E;AACxE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,QAAQ,GACV,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD,CADJ;;AAEA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,eAAO,EAAE,CAAC,GAAH,CAAO,UAAP,EAAmB,OAAnB,CAA2B,EAAE,CAAC,KAA9B,CAAP;AACD;;AACD,aAAO,EAAP;AACD,KAND;;AAOA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,EAAP,EAAW,KAAX,GAAmB,GAAnB,EAAP,CAAZ;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,eAAO,GAAG,CAAC,GAAJ,CAAQ,UAAR,EAAoB,OAApB,CAA4B,EAAE,CAAC,KAA/B,CAAP;AACD;;AACD,aAAO,GAAP;AACD,KAPD;;AAQA,WAAO;AAAC,MAAA,EAAE,EAAE,IAAL;AAAW,MAAA,EAAE,EAAE;AAAf,KAAP;AACD,GAlBD;;AAmBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,EAAgB,EAAhB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,EAAD;AAAK,IAAA;AAAL,GAJI,EAIM,GAJN,CAAP;AAKD;AAED;;;;;;;;;;AAQA,SAAS,UAAT,CAAsC,CAAtC,EAAuD,CAAvD,EAAsE;AACpE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,sBAA3C;AACA,SAAO,EAAE,CAAC,GAAH,CAAO,EAAP,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;AAyBA;;;AACA,SAAS,QAAT,CACI,CADJ,EAC0B,CAD1B,EAC8C;AAC5C,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACA,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACD;;AAED,EAAA,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD;;AACA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,SAAH,CAAa,EAAb,EAAiB,OAAjB,EAAP,CAAnB;;AACA,UAAM,IAAI,GAAG,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,CAAW,EAAX,EAAe,OAAf,EAAP,CAAnB;;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,IAAJ;AAAU,MAAA,CAAC,EAAE;AAAb,KAAP;AACD,GALD;;AAMA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,OAAR,CAAgB,EAAhB,EAAoB,EAApB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,EAAJ;AAAQ,IAAA,CAAC,EAAE;AAAX,GAJI,EAIY,GAJZ,EAIiB,SAJjB,CAAP;AAKD;AAED;;;;;;;;;;AAQA,SAAS,cAAT,CAA0C,CAA1C,EAA2D,CAA3D,EAA0E;AACxE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,eAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,eAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,0BAA3C;AACA,SAAO,EAAE,CAAC,OAAH,CAAW,EAAX,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;AAyBA;;;AACA,SAAS,QAAT,CACI,CADJ,EAC0B,CAD1B,EAC8C;AAC5C,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACA,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACD;;AAED,EAAA,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD;;AACA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,YAAH,CAAgB,EAAhB,EAAoB,OAApB,EAAP,CAAnB;;AACA,UAAM,IAAI,GAAG,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,IAAH,CAAQ,EAAR,EAAY,OAAZ,EAAP,CAAnB;;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,IAAJ;AAAU,MAAA,CAAC,EAAE;AAAb,KAAP;AACD,GALD;;AAMA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,OAAR,CAAgB,EAAhB,EAAoB,EAApB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE,EAAJ;AAAQ,IAAA,CAAC,EAAE;AAAX,GAJI,EAIY,GAJZ,EAIiB,SAJjB,CAAP;AAKD;AAED;;;;;;;;;;AAQA,SAAS,cAAT,CAA0C,CAA1C,EAA2D,CAA3D,EAA0E;AACxE,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAIA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,eAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,eAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CAAuB,EAAE,CAAC,KAA1B,EAAiC,EAAE,CAAC,KAApC,EAA2C,0BAA3C;AACA,SAAO,EAAE,CAAC,OAAH,CAAW,EAAX,CAAP;AACD;AAED;;;;;;;;;;;;AAUA,SAAS,wBAAT,CACI,CADJ,EACqB,CADrB,EACoC;AAClC,EAAA,eAAe,CACX,iDACA,+BAFW,CAAf;AAGA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,yBAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,yBAAT,CAA1B;AACA,EAAA,IAAI,CAAC,iBAAL,CACI,EAAE,CAAC,KADP,EACc,EAAE,CAAC,KADjB,EACwB,oCADxB;AAEA,SAAO,EAAE,CAAC,iBAAH,CAAqB,EAArB,CAAP;AACD;AAED;;;;;;;;;;;;;;;;AAeA;;;AACA,SAAS,MAAT,CACI,CADJ,EAC0B,CAD1B,EAC8C;AAC5C,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,QAAQ,GACV,cAAc,CAAC,0BAAf,CAA0C,EAAE,CAAC,KAA7C,EAAoD,EAAE,CAAC,KAAvD,CADJ;;AAGA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAa,KAAb,KAAgC;AAC1C,UAAM,CAAC,EAAD,EAAK,EAAL,IAAW,KAAjB;;AACA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,CAAC,GAAG,GAAG,CAAC,EAAE,CAAC,MAAH,EAAD,EAAc,EAAE,CAAC,MAAH,EAAd,CAAb;AACA,UAAI,GAAG,GAAG,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,CAAP,CAAP,CAAV;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,aAAO,GAAG,CAAC,OAAJ,CAAY,EAAE,CAAC,KAAf,CAAP;AACD,KARD;;AASA,UAAM,IAAI,GAAG,MAAK;AAChB,YAAM,CAAC,GAAG,GAAG,CAAC,EAAE,CAAC,MAAH,EAAD,EAAc,EAAE,CAAC,MAAH,EAAd,CAAb;AACA,UAAI,GAAG,GAAG,GAAG,CAAC,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,CAAP,CAAP,CAAD,CAAb;AACA,YAAM,UAAU,GAAG,cAAc,CAAC,gBAAf,CAAgC,EAAE,CAAC,KAAnC,EAA0C,QAA1C,CAAnB;;AACA,UAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,QAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,aAAO,GAAG,CAAC,OAAJ,CAAY,EAAE,CAAC,KAAf,CAAP;AACD,KARD;;AASA,WAAO;AAAC,MAAA,EAAE,EAAE,IAAL;AAAW,MAAA,EAAE,EAAE;AAAf,KAAP;AACD,GArBD;;AAsBA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,EAAkB,EAAlB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,EAAK,EAAL,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,EAAD;AAAK,IAAA;AAAL,GAJI,EAIM,GAJN,CAAP;AAKD;;AAED,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,QAAQ,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAnB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,aAAa,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAxB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,aAAa,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAxB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,uBAAuB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlC;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { deprecationWarn } from '../globals';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { add } from './add';\nimport * as broadcast_util from './broadcast_util';\nimport { op } from './operation';\nimport { scalar, zerosLike } from './tensor_ops';\nimport { neg } from './unary_ops';\n/**\n * @deprecated\n * Adds two `tf.Tensor`s element-wise, A + B.\n *\n * Inputs must be the same shape. For broadcasting support, use add() instead.\n *\n * @param a The first Tensor to add element-wise.\n * @param b The second Tensor to add element-wise.\n */\nfunction addStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'addStrict');\n    const $b = convertToTensor(b, 'b', 'addStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in addStrict: ');\n    return $a.add($b);\n}\n/**\n * @deprecated\n * Subtracts two `tf.Tensor`s element-wise, A - B. Inputs must\n * be the same shape.\n *\n * For broadcasting support, use `tf.sub` instead.\n *\n * @param a The first Tensor to subtract element-wise.\n * @param b The second Tensor to subtract element-wise.\n */\nfunction subStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'subStrict');\n    const $b = convertToTensor(b, 'b', 'subStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in subStrict: ');\n    return $a.sub($b);\n}\n/**\n * Computes the power of one `tf.Tensor` to another. Supports broadcasting.\n *\n * Given a `tf.Tensor` x and a `tf.Tensor` y, this operation computes x^y for\n * corresponding elements in x and y. The result's dtype will be the upcasted\n * type of the `base` and `exp` dtypes.\n *\n * ```js\n * const a = tf.tensor([[2, 3], [4, 5]])\n * const b = tf.tensor([[1, 2], [3, 0]]).toInt();\n *\n * a.pow(b).print();  // or tf.pow(a, b)\n * ```\n *\n * ```js\n * const a = tf.tensor([[1, 2], [3, 4]])\n * const b = tf.tensor(2).toInt();\n *\n * a.pow(b).print();  // or tf.pow(a, b)\n * ```\n * We also expose `powStrict` which has the same signature as this op and\n * asserts that `base` and `exp` are the same shape (does not broadcast).\n *\n * @param base The base `tf.Tensor` to pow element-wise.\n * @param exp The exponent `tf.Tensor` to pow element-wise.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction pow_(base, exp) {\n    let $base = convertToTensor(base, 'base', 'pow');\n    let $exp = convertToTensor(exp, 'exp', 'pow');\n    [$base, $exp] = makeTypesMatch($base, $exp);\n    const outShape = broadcast_util.assertAndGetBroadcastShape($base.shape, $exp.shape);\n    const grad = (dy, saved) => {\n        const [$base, $exp, y] = saved;\n        const derBase = () => {\n            const expFloat = $exp.toFloat();\n            let res = dy.mul(expFloat.mul($base.pow(expFloat.sub(scalar(1)))));\n            const reduceAxes = broadcast_util.getReductionAxes($base.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = res.sum(reduceAxes);\n            }\n            return res.reshape($base.shape);\n        };\n        const derExp = () => {\n            const condition = $base.greater(0);\n            const logBase = $base.log().where(condition, zerosLike($base));\n            let res = dy.mul(y.mul(logBase));\n            const reduceAxes = broadcast_util.getReductionAxes($exp.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = res.sum(reduceAxes);\n            }\n            return res.reshape($exp.shape);\n        };\n        return { a: derBase, b: derExp };\n    };\n    const attrs = {};\n    const inputsToSave = [$base, $exp];\n    const outputsToSave = [true];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.pow($base, $exp);\n        save([$base, $exp, y]);\n        return y;\n    }, { a: $base, b: $exp }, grad, 'Pow', attrs, inputsToSave, outputsToSave);\n}\n/**\n * @deprecated\n * Computes the power of one `tf.Tensor` to another. Inputs must\n * be the same shape.\n *\n * For broadcasting support, use `tf.pow` instead.\n *\n * @param base The base tensor to pow element-wise.\n * @param exp The exponent tensor to pow element-wise.\n */\nfunction powStrict_(base, exp) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    util.assertShapesMatch(base.shape, exp.shape, 'Error in powStrict: ');\n    return base.pow(exp);\n}\n/**\n * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.\n *\n * We also expose `tf.mulStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.tensor1d([2, 3, 4, 5]);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n *\n * ```js\n * // Broadcast mul a with b.\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.scalar(5);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n * @param a The first tensor to multiply.\n * @param b The second tensor to multiply. Must have the same dtype as `a`.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction mul_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mul');\n    let $b = convertToTensor(b, 'b', 'mul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => {\n            const res = dy.mul($b.toFloat());\n            const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return res.sum(reduceAxes).reshape($a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            const res = dy.mul($a.toFloat());\n            const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return res.sum(reduceAxes).reshape($b.shape);\n            }\n            return res;\n        };\n        return { a: derA, b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.multiply($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { a: $a, b: $b }, der, 'Mul');\n}\n/**\n * @deprecated\n * Multiplies two `tf.Tensor`s element-wise, A * B.\n *\n * Inputs must be the same shape. For broadcasting support, use `tf.mul`.\n *\n * @param a The first tensor to multiply.\n * @param b The first tensor to multiply. Must have the same\n *    dtype as `a`.\n */\nfunction mulStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'mul');\n    const $b = convertToTensor(b, 'b', 'mul');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in multiplyStrict: ');\n    return $a.mul($b);\n}\n/**\n * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.\n * The result is rounded with floor function.\n *\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 9, 16]);\n * const b = tf.tensor1d([1, 2, 3, 4]);\n *\n * a.floorDiv(b).print();  // or tf.div(a, b)\n * ```\n *\n * ```js\n * // Broadcast div a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(2);\n *\n * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n * ```\n *\n * @param a The first tensor as the numerator.\n * @param b The second tensor as the denominator. Must have the same dtype as\n * `a`.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction floorDiv_(a, b) {\n    let $a = convertToTensor(a, 'a', 'floorDiv');\n    let $b = convertToTensor(b, 'b', 'floorDiv');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => {\n            const res = dy.div($b.toFloat());\n            const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return res.sum(reduceAxes).reshape($a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            let res = dy.mul($a.toFloat());\n            const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = res.sum(reduceAxes).reshape($b.shape);\n            }\n            const tmp = $b.square();\n            return res.div(tmp.toFloat()).neg();\n        };\n        return { a: derA, b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.floorDiv($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { a: $a, b: $b }, der, 'FloorDiv');\n}\n/**\n * @deprecated\n * Divides two `tf.Tensor`s element-wise, A / B. Inputs must\n * be the same shape.\n *\n * @param a The first tensor as the numerator for element-wise division.\n * @param b The second tensor as the denominator for element-wise division.\n */\nfunction divStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'div');\n    const $b = convertToTensor(b, 'b', 'div');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in divideStrict: ');\n    return $a.div($b);\n}\n/**\n * Returns the mod of a and b element-wise.\n * `floor(x / y) * y + mod(x, y) = x`\n * Supports broadcasting.\n *\n * We also expose `tf.modStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * ```js\n * // Broadcast a mod b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction mod_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mod');\n    let $b = convertToTensor(b, 'b', 'mod');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => {\n            const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return dy.sum(reduceAxes).reshape($a.shape);\n            }\n            return dy;\n        };\n        const derB = () => {\n            const res = dy.mul($a.div($b).floor().neg());\n            const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return res.sum(reduceAxes).reshape($b.shape);\n            }\n            return res;\n        };\n        return { $a: derA, $b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.mod($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { $a, $b }, der);\n}\n/**\n * @deprecated\n * Returns the mod of a and b (`a < b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use mod().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\nfunction modStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'modStrict');\n    const $b = convertToTensor(b, 'b', 'modStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in modStrict: ');\n    return $a.mod($b);\n}\n/**\n * Returns the min of a and b (`a < b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `minimumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * ```js\n * // Broadcast minimum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction minimum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'minimum');\n    let $b = convertToTensor(b, 'b', 'minimum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = $a.toInt();\n        $b = $b.toInt();\n    }\n    broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => dy.mul($a.lessEqual($b).toFloat());\n        const derB = () => dy.mul($a.greater($b).toFloat());\n        return { a: derA, b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.minimum($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { a: $a, b: $b }, der, 'Minimum');\n}\n/**\n * @deprecated\n * Returns the min of a and b (`a < b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use minimum().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\nfunction minimumStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'minimumStrict');\n    const $b = convertToTensor(b, 'b', 'minimumStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in minimumStrict: ');\n    return $a.minimum($b);\n}\n/**\n * Returns the max of a and b (`a > b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `tf.maximumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * ```js\n * // Broadcast maximum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\n/** @doc {heading: 'Operations', subheading: 'Arithmetic'} */\nfunction maximum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'maximum');\n    let $b = convertToTensor(b, 'b', 'maximum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = $a.toInt();\n        $b = $b.toInt();\n    }\n    broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => dy.mul($a.greaterEqual($b).toFloat());\n        const derB = () => dy.mul($a.less($b).toFloat());\n        return { a: derA, b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.maximum($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { a: $a, b: $b }, der, 'Maximum');\n}\n/**\n * @deprecated\n * Returns the max of a and b (`a > b ? a : b`) element-wise. Inputs must\n * be the same shape. For broadcasting support, use maximum().\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n */\nfunction maximumStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'maximumStrict');\n    const $b = convertToTensor(b, 'b', 'maximumStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in maximumStrict: ');\n    return $a.maximum($b);\n}\n/**\n * @deprecated\n * Returns (a - b) * (a - b) element-wise.\n *\n * Inputs must be the same shape. For broadcasting support, use\n * `tf.squaredDifference` instead.\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n */\nfunction squaredDifferenceStrict_(a, b) {\n    deprecationWarn('strict variants of ops have been deprecated ' +\n        'and will be removed in future');\n    const $a = convertToTensor(a, 'a', 'squaredDifferenceStrict');\n    const $b = convertToTensor(b, 'b', 'squaredDifferenceStrict');\n    util.assertShapesMatch($a.shape, $b.shape, 'Error in squaredDifferenceStrict: ');\n    return $a.squaredDifference($b);\n}\n/**\n * Computes arctangent of `tf.Tensor`s a / b element-wise: `atan2(a, b)`.\n * Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1.0, 1.0, -1.0, .7]);\n * const b = tf.tensor1d([2.0, 13.0, 3.5, .21]);\n *\n * tf.atan2(a, b).print()\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same dtype as `a`.\n *\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction atan2_(a, b) {\n    let $a = convertToTensor(a, 'a', 'atan2');\n    let $b = convertToTensor(b, 'b', 'atan2');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const outShape = broadcast_util.assertAndGetBroadcastShape($a.shape, $b.shape);\n    const der = (dy, saved) => {\n        const [$a, $b] = saved;\n        const derA = () => {\n            const d = add($a.square(), $b.square());\n            let res = dy.mul($b.div(d));\n            const reduceAxes = broadcast_util.getReductionAxes($a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = res.sum(reduceAxes);\n            }\n            return res.reshape($a.shape);\n        };\n        const derB = () => {\n            const d = add($a.square(), $b.square());\n            let res = neg(dy.mul($a.div(d)));\n            const reduceAxes = broadcast_util.getReductionAxes($b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = res.sum(reduceAxes);\n            }\n            return res.reshape($b.shape);\n        };\n        return { $a: derA, $b: derB };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.atan2($a, $b);\n        save([$a, $b]);\n        return res;\n    }, { $a, $b }, der);\n}\nexport const addStrict = op({ addStrict_ });\nexport const atan2 = op({ atan2_ });\nexport const divStrict = op({ divStrict_ });\nexport const floorDiv = op({ floorDiv_ });\nexport const maximum = op({ maximum_ });\nexport const maximumStrict = op({ maximumStrict_ });\nexport const minimum = op({ minimum_ });\nexport const minimumStrict = op({ minimumStrict_ });\nexport const mod = op({ mod_ });\nexport const modStrict = op({ modStrict_ });\nexport const mul = op({ mul_ });\nexport const mulStrict = op({ mulStrict_ });\nexport const pow = op({ pow_ });\nexport const powStrict = op({ powStrict_ });\nexport const squaredDifferenceStrict = op({ squaredDifferenceStrict_ });\nexport const subStrict = op({ subStrict_ });\n//# sourceMappingURL=binary_ops.js.map"]},"metadata":{},"sourceType":"module"}